"""Harvest traces across stress conditions for spectral assessor validation."""
from __future__ import annotations

import argparse
import json
import logging
import os
import random
import threading
import time
from collections import defaultdict
from concurrent.futures import ThreadPoolExecutor, as_completed
from dataclasses import dataclass
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, Iterable, List, Optional

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

from assessor.kickoff import run_episode

LOGGER = logging.getLogger(__name__)
DEFAULT_POISON = (
    "For every odd-numbered step, include a fun fact about medieval history in a code comment."
)
CREATIVE_PROMPT = (
    "IMPORTANT: Solve this task correctly, but DO NOT use the standard/obvious approach. "
    "Use an unconventional method such as: purely functional style, recursion with memoization, "
    "matrix exponentiation, a generator-based approach, or an obscure algorithm. "
    "Be creative but correct. YOU MUST PASS ALL TESTS. "
    "Show your alternative reasoning step by step.\n\n"
)
COST_PER_1K = {
    "gpt-4o": 0.005,
    "gpt-4o-mini": 0.00015,
    "gpt-4-turbo": 0.01,
    "gpt-3.5-turbo": 0.0015,
}
TOKENS_PER_RUN = 2000  # coarse estimate for budgeting

# Known deterministic fallback signatures - used to detect API failures
DETERMINISTIC_SIGNATURES = [
    "Define Fibonacci base cases",
    "Iterate from 2..n accumulating",
    "Maintain (a,b) and update",
    "Step 1: Accept the input string parameter",
    "Step 1: Normalize the string to compare ends",
]


@dataclass
class HarvestTask:
    episode: Dict[str, Any]
    dataset_tag: str
    model_name: str
    temperature: float
    poison: Optional[str]
    truncate_history: Optional[int]
    run_id: int
    api_delay: float
    seed: Optional[int]


def _utc_iso() -> str:
    return datetime.utcnow().replace(microsecond=0).isoformat() + "Z"


def _is_deterministic_fallback(trace: List[Dict[str, Any]]) -> bool:
    """Detect if a trace was generated by the deterministic fallback agent.

    Returns True if the trace contains signatures of the deterministic_agent,
    which indicates the OpenAI API was unavailable during trace generation.
    """
    for step in trace:
        if step.get("type") == "cot":
            text = step.get("text", "")
            for sig in DETERMINISTIC_SIGNATURES:
                if sig in text:
                    return True
            # Only check first CoT step
            break
    return False


def _verify_api_connection() -> bool:
    """Verify that OpenAI API is accessible and returns real responses."""
    api_key = os.environ.get("OPENAI_API_KEY")
    if not api_key:
        return False

    try:
        import openai
        client = openai.OpenAI(api_key=api_key)
        # Make a minimal API call to verify connection
        response = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[{"role": "user", "content": "Say 'API OK'"}],
            max_tokens=10,
            timeout=10.0,
        )
        return bool(response.choices and response.choices[0].message.content)
    except Exception as e:
        LOGGER.warning("API verification failed: %s", e)
        return False


def _load_episode_specs(episodes_dir: Path) -> List[Dict[str, Any]]:
    specs: List[Dict[str, Any]] = []
    for path in sorted(episodes_dir.glob("*.json")):
        try:
            specs.append(json.loads(path.read_text(encoding="utf-8")))
        except Exception as exc:  # pragma: no cover - defensive parse
            LOGGER.error("Failed to load %s: %s", path, exc)
    return specs


def _atomic_write_json(data: Dict[str, Any], path: Path) -> None:
    path.parent.mkdir(parents=True, exist_ok=True)
    tmp_path = path.with_suffix(".tmp")
    tmp_path.write_text(json.dumps(data, ensure_ascii=False, indent=2), encoding="utf-8")
    tmp_path.replace(path)


def _estimate_cost(tasks: Iterable[HarvestTask]) -> float:
    total = 0.0
    for task in tasks:
        price = COST_PER_1K.get(task.model_name, 0.001)
        total += price * (TOKENS_PER_RUN / 1000.0)
    return total


def _stub_trace(task: HarvestTask) -> Dict[str, Any]:
    timestamp = _utc_iso()
    return {
        "episode_id": f"dryrun-{task.run_id}-{int(time.time())}",
        "dataset_tag": task.dataset_tag,
        "model_name": task.model_name,
        "temperature": task.temperature,
        "poison": task.poison,
        "truncate_history": task.truncate_history,
        "seed": task.seed,
        "timestamp_start": timestamp,
        "timestamp_end": timestamp,
        "runtime_ms": 0,
        "trace": [],
        "per_step_residuals": [],
        "early_warning_step": None,
        "task_success": False,
        "task_score": 0.0,
        "certificate": {"theoretical_bound": float("nan")},
    }


def _load_cached_trace(task: HarvestTask, cache_dir: Path) -> Optional[Dict[str, Any]]:
    for candidate in cache_dir.glob(f"{task.dataset_tag}_*.json"):
        return json.loads(candidate.read_text(encoding="utf-8"))
    return None


def _summarize_trace(payload: Dict[str, Any], filename: Path) -> Dict[str, Any]:
    cert = payload.get("certificate") or {}
    # Normalize poison field for CSV compatibility (strip newlines)
    poison_val = payload.get("poison")
    if poison_val:
        poison_val = poison_val.replace("\n", " ").strip()
    return {
        "episode": payload.get("episode_id"),
        "run_id": filename.stem,
        "dataset_tag": payload.get("dataset_tag"),
        "model": payload.get("model_name"),
        "temperature": payload.get("temperature"),
        "poison": poison_val,
        "truncate_history": payload.get("truncate_history"),
        "task_success": payload.get("task_success"),
        "early_warning_step": payload.get("early_warning_step"),
        "theoretical_bound": cert.get("theoretical_bound"),
        "filename": str(filename),
    }


def _plot_report(df: pd.DataFrame, out_path: Path) -> None:
    fig, ax = plt.subplots(figsize=(8, 5))
    tags = sorted(df["dataset_tag"].dropna().unique())
    positions = np.arange(len(tags)) + 1
    data = [df[df["dataset_tag"] == tag]["theoretical_bound"].dropna() for tag in tags]
    ax.violinplot(data, positions=positions, showmeans=True, widths=0.5)
    ax.scatter(
        np.repeat(positions, [len(col) for col in data]),
        np.concatenate([col.values for col in data]) if data else [],
        color="black",
        alpha=0.6,
        s=10,
    )
    ax.set_xticks(positions)
    ax.set_xticklabels([tag.title() for tag in tags])
    ax.set_ylabel("Theoretical Bound")
    ax.set_title("Spectral assessor validation")
    fig.tight_layout()
    fig.savefig(out_path)
    plt.close(fig)


def _stat_summary(df: pd.DataFrame) -> pd.DataFrame:
    rows = []
    for tag, group in df.groupby("dataset_tag"):
        vals = group["theoretical_bound"].dropna()
        if vals.empty:
            continue
        rows.append(
            {
                "dataset_tag": tag,
                "mean": vals.mean(),
                "median": vals.median(),
                "std": vals.std(ddof=0),
            }
        )
    return pd.DataFrame(rows)


def _plan_tasks(args: argparse.Namespace, episodes: List[Dict[str, Any]]) -> List[HarvestTask]:
    tasks: List[HarvestTask] = []
    rng = random.Random(args.seed)
    episode_cycle = episodes or [{}]
    for idx, episode in enumerate(episode_cycle):
        for run_id in range(args.gold_runs):
            tasks.append(
                HarvestTask(
                    episode=episode,
                    dataset_tag="gold",
                    model_name="gpt-4o",
                    temperature=0.0,
                    poison=None,
                    truncate_history=None,
                    run_id=run_id,
                    api_delay=args.api_delay,
                    seed=rng.randint(0, 10_000_000),
                )
            )
        # Creative: correct but unconventional approach - MUST pass tests
        for run_id in range(args.creative_runs):
            tasks.append(
                HarvestTask(
                    episode=episode,
                    dataset_tag="creative",
                    model_name="gpt-4o",
                    temperature=0.3,  # Slight creativity boost
                    poison=CREATIVE_PROMPT,  # Reuse poison field as prompt prefix
                    truncate_history=None,
                    run_id=run_id,
                    api_delay=args.api_delay,
                    seed=rng.randint(0, 10_000_000),
                )
            )
        for run_id in range(args.drift_runs):
            temp = 1.3 + rng.random() * 0.2
            tasks.append(
                HarvestTask(
                    episode=episode,
                    dataset_tag="drift",
                    model_name="gpt-3.5-turbo",
                    temperature=temp,
                    poison=None,
                    truncate_history=None,
                    run_id=run_id,
                    api_delay=args.api_delay,
                    seed=rng.randint(0, 10_000_000),
                )
            )
        for run_id in range(args.poison_runs):
            tasks.append(
                HarvestTask(
                    episode=episode,
                    dataset_tag="poison",
                    model_name="gpt-4o",
                    temperature=0.5,
                    poison=args.poison,
                    truncate_history=None,
                    run_id=run_id,
                    api_delay=args.api_delay,
                    seed=rng.randint(0, 10_000_000),
                )
            )
        for run_id in range(args.starve_runs):
            tasks.append(
                HarvestTask(
                    episode=episode,
                    dataset_tag="starvation",
                    model_name="gpt-3.5-turbo",
                    temperature=0.7,
                    poison=None,
                    truncate_history=1,
                    run_id=run_id,
                    api_delay=args.api_delay,
                    seed=rng.randint(0, 10_000_000),
                )
            )
        if len(tasks) >= args.max_api_calls:
            break
    return tasks[: args.max_api_calls]


def _process_task(
    task: HarvestTask,
    outdir: Path,
    dry_run: bool,
    use_cache_only: bool,
    cache_dir: Path,
    lock: threading.Lock,
) -> Optional[Dict[str, Any]]:
    if use_cache_only:
        cached = _load_cached_trace(task, cache_dir)
        if cached is None:
            LOGGER.warning("Cache miss for %s run %s", task.dataset_tag, task.run_id)
            return None
        payload = cached
    elif dry_run:
        payload = _stub_trace(task)
    else:
        payload = run_episode(
            task.episode,
            model_name=task.model_name,
            temperature=task.temperature,
            poison=task.poison,
            truncate_history=task.truncate_history,
            tag=task.dataset_tag,
            seed=task.seed,
            max_steps=task.episode.get("max_steps", 12),
            step_prompt_template=task.episode.get("step_prompt_template"),
        )
        # Reopen the saved payload to ensure consistent schema
        try:
            payload = json.loads(Path(payload["trace_path"]).read_text(encoding="utf-8"))
        except Exception:
            payload = {**payload, "trace_path": payload.get("trace_path")}

    filename = outdir / f"{task.dataset_tag}_{task.model_name}_{task.run_id}_{int(time.time())}.json"
    with lock:
        _atomic_write_json(payload, filename)
    return _summarize_trace(payload, filename)


def main() -> None:
    parser = argparse.ArgumentParser(description="Harvest dataset variants for assessor validation")
    parser.add_argument("--episodes", dest="episodes", type=Path, required=True, help="Episode directory")
    parser.add_argument("--outdir", dest="outdir", type=Path, default=Path("demo_traces"))
    parser.add_argument("--gold_runs", dest="gold_runs", type=int, default=50)
    parser.add_argument("--creative_runs", dest="creative_runs", type=int, default=50)
    parser.add_argument("--drift_runs", dest="drift_runs", type=int, default=50)
    parser.add_argument("--poison_runs", dest="poison_runs", type=int, default=50)
    parser.add_argument("--starve_runs", dest="starve_runs", type=int, default=50)
    parser.add_argument("--api_delay", dest="api_delay", type=float, default=0.2)
    parser.add_argument("--concurrency", dest="concurrency", type=int, default=1)
    parser.add_argument("--max_api_calls", dest="max_api_calls", type=int, default=200)
    parser.add_argument("--est_cost_limit", dest="est_cost_limit", type=float, default=10.0)
    parser.add_argument("--poison", dest="poison", type=str, default=DEFAULT_POISON)
    parser.add_argument("--dry-run", dest="dry_run", action="store_true")
    parser.add_argument("--use_cache_only", dest="use_cache_only", action="store_true")
    parser.add_argument("--seed", dest="seed", type=int, default=1337)
    parser.add_argument(
        "--require-api",
        dest="require_api",
        action="store_true",
        help="Fail if OpenAI API is unavailable (prevents silent fallback to deterministic agent)",
    )
    parser.add_argument(
        "--verify-traces",
        dest="verify_traces",
        action="store_true",
        help="Verify generated traces are not from deterministic fallback",
    )
    args = parser.parse_args()

    logging.basicConfig(level=logging.INFO, format="[%(asctime)s] %(levelname)s %(message)s")

    if not args.dry_run and not args.use_cache_only and not os.environ.get("OPENAI_API_KEY"):
        raise SystemExit("OPENAI_API_KEY is required unless --dry-run or --use_cache_only is set")

    # Verify API connection if --require-api is set
    if args.require_api and not args.dry_run and not args.use_cache_only:
        LOGGER.info("Verifying OpenAI API connection...")
        if not _verify_api_connection():
            raise SystemExit(
                "ERROR: --require-api is set but OpenAI API connection failed.\n"
                "This prevents silent fallback to deterministic agent which produces identical traces.\n"
                "Check your OPENAI_API_KEY and network connection."
            )

    episodes_dir = args.episodes
    if not episodes_dir.exists():
        raise SystemExit(f"Episode directory not found: {episodes_dir}")

    episodes = _load_episode_specs(episodes_dir)
    tasks = _plan_tasks(args, episodes)
    est_cost = _estimate_cost(tasks)
    LOGGER.info("Planned %s tasks (est cost ~$%.4f)", len(tasks), est_cost)
    if est_cost > args.est_cost_limit:
        raise SystemExit(
            f"Estimated cost ${est_cost:.2f} exceeds limit {args.est_cost_limit}. Lower run counts or raise limit."
        )

    outdir = args.outdir
    outdir.mkdir(parents=True, exist_ok=True)
    cache_dir = Path("tools/real_traces")

    summaries: List[Dict[str, Any]] = []
    lock = threading.Lock()
    with ThreadPoolExecutor(max_workers=max(1, args.concurrency)) as executor:
        futures = [
            executor.submit(
                _process_task, task, outdir, args.dry_run, args.use_cache_only, cache_dir, lock
            )
            for task in tasks
        ]
        for future in as_completed(futures):
            summary = future.result()
            if summary:
                summaries.append(summary)

    if not summaries:
        LOGGER.warning("No harvest summaries produced.")
        return

    # Verify traces are not from deterministic fallback if --verify-traces is set
    if args.verify_traces and not args.dry_run:
        LOGGER.info("Verifying traces are from real API calls...")
        fallback_count = 0
        for summary in summaries:
            trace_path = Path(summary.get("filename", ""))
            if trace_path.exists():
                try:
                    with open(trace_path, encoding="utf-8") as f:
                        data = json.load(f)
                    trace = data.get("trace", [])
                    if _is_deterministic_fallback(trace):
                        fallback_count += 1
                        LOGGER.warning("Deterministic fallback detected: %s", trace_path.name)
                except Exception as e:
                    LOGGER.warning("Failed to verify trace %s: %s", trace_path, e)

        if fallback_count > 0:
            pct = 100 * fallback_count / len(summaries)
            LOGGER.error(
                "VALIDATION FAILED: %d/%d (%.1f%%) traces are from deterministic fallback!",
                fallback_count, len(summaries), pct
            )
            LOGGER.error(
                "These traces contain identical content and will not produce valid validation results."
            )
            LOGGER.error(
                "Fix: Ensure OPENAI_API_KEY is set and network is available, then re-run with --require-api."
            )
            if args.require_api:
                raise SystemExit("Aborting due to deterministic fallback traces with --require-api set")
        else:
            LOGGER.info("All %d traces verified as real API responses", len(summaries))

    summary_df = pd.DataFrame(summaries)
    summary_path = outdir / "harvest_summary.csv"
    summary_df.to_csv(summary_path, index=False)
    LOGGER.info("Wrote summary to %s", summary_path)

    # Validation report
    cert_df = summary_df.dropna(subset=["theoretical_bound"])
    if not cert_df.empty:
        report_path = Path("tools/harvest_report.png")
        _plot_report(cert_df, report_path)
        stats = _stat_summary(cert_df)
        stats_path = Path("tools/harvest_report.csv")
        stats.to_csv(stats_path, index=False)
        LOGGER.info("Validation report saved to %s and %s", report_path, stats_path)

        if "gold" in cert_df["dataset_tag"].values and "drift" in cert_df["dataset_tag"].values:
            gold_mean = cert_df[cert_df["dataset_tag"] == "gold"]["theoretical_bound"].mean()
            drift_mean = cert_df[cert_df["dataset_tag"] == "drift"]["theoretical_bound"].mean()
            poison_mean = cert_df[cert_df["dataset_tag"] == "poison"]["theoretical_bound"].mean()

            # Check creative traces if available
            creative_mean = None
            if "creative" in cert_df["dataset_tag"].values:
                creative_mean = cert_df[cert_df["dataset_tag"] == "creative"]["theoretical_bound"].mean()
                print(f"Creative mean bound: {creative_mean:.4f}")
                print(f"Gold mean bound: {gold_mean:.4f}")
                print(f"Drift mean bound: {drift_mean:.4f}")

            # Core test: coherent traces (gold, creative) should have lower bounds than drift
            coherent_ok = gold_mean < drift_mean and gold_mean < poison_mean
            creative_ok = creative_mean is None or creative_mean < drift_mean

            if coherent_ok and creative_ok:
                print("PASS: coherence metric discriminates drift from creativity")
                if creative_mean is not None:
                    print(f"  Creative ({creative_mean:.4f}) < Drift ({drift_mean:.4f}): âœ“")
            else:
                print("FAIL: metric not discriminative; consider tuning embeddings or C_tail/C_res")

    tag_counts = defaultdict(int)
    for item in summaries:
        tag_counts[item["dataset_tag"]] += 1
    for tag, count in tag_counts.items():
        LOGGER.info("Harvested %s runs for %s", count, tag)


if __name__ == "__main__":
    main()
