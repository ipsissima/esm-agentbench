# This file is for reproducible developer installs; the canonical package metadata is pyproject.toml.
#
# CI Installation:
#   pip install -r requirements.txt --extra-index-url https://download.pytorch.org/whl/cpu
#
# Local (with GPU):
#   pip install -r requirements.txt
#   pip install torch  # will get CUDA version from PyPI

# Core scientific stack (pinned for reproducibility)
numpy==1.26.2
scipy==1.11.4
scikit-learn==1.3.2
joblib==1.3.2
pandas==2.2.3

# Web framework
flask==3.0.0
blinker>=1.6.2
werkzeug>=3.0.0
gunicorn==20.1.0

# Data validation
pydantic==2.5.0
PyYAML==6.0.1

# Visualization
matplotlib==3.8.1
seaborn==0.13.0

# Testing
pytest==7.4.3
pytest-asyncio==0.21.1
pytest-timeout>=2.2.0
tomli==2.0.1

# Notebooks
jupyter==1.0.0
nbconvert==7.11.0

# Signing/verification
PyNaCl==1.5.0

# OpenAI (for API-based evaluation)
openai==1.3.0

# ML/NLP stack - HuggingFace ecosystem
# Note: For CI, install with --extra-index-url https://download.pytorch.org/whl/cpu
# to get CPU-only torch (~200MB instead of ~2GB CUDA version)
# Version constraint without +cpu suffix - pip resolves CPU wheels from extra-index-url
torch>=2.2.0
transformers>=4.40.0
accelerate==0.25.0
huggingface-hub>=0.19.3
sentence-transformers==2.3.1

# Optional: for quantization (install if you have GPU and want 4-bit models)
# bitsandbytes==0.41.3

# Optional: for faster inference (Linux + CUDA only)
# vllm==0.2.7
