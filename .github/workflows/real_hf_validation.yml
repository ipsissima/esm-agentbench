name: Real HF Agent Validation

# This workflow tests the real agent HF evaluation pipeline.
# It's optional and only runs when RUN_REAL_HF=1 or on manual dispatch.
# Uses phi-3-mini-instruct for efficient pipeline validation with proper context.

on:
  workflow_dispatch:
    inputs:
      scenario:
        description: 'Scenario to test (or "all" for all scenarios)'
        required: false
        default: 'supply_chain_poisoning'
      n_runs:
        description: 'Number of runs per label'
        required: false
        default: '2'

  push:
    branches:
      - main
      - 'claude/**'
    paths:
      - 'tools/real_agents_hf/**'
      - 'analysis/run_real_hf_experiment.py'
      - 'scenarios/*/real_agent_prompts/**'
      - '.github/workflows/real_hf_validation.yml'

  pull_request:
    paths:
      - 'tools/real_agents_hf/**'
      - 'analysis/run_real_hf_experiment.py'

env:
  PYTHON_VERSION: '3.11'
  # Set to 1 to enable HF evaluation (requires time and resources)
  RUN_REAL_HF: ${{ secrets.RUN_REAL_HF || '0' }}
  # Skip verified kernel (requires Coq/OCaml build) and use Python fallback
  ESM_SKIP_VERIFIED_KERNEL: "1"

jobs:
  test-pipeline:
    name: Test Real HF Pipeline
    runs-on: ubuntu-latest
    timeout-minutes: 30

    # Only run if RUN_REAL_HF=1 or manual dispatch
    if: github.event_name == 'workflow_dispatch' || vars.RUN_REAL_HF == '1'

    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m venv .venv
          . .venv/bin/activate
          python -m pip install --upgrade pip
          python -m pip uninstall -y huggingface-hub sentence-transformers transformers accelerate || true
          # canonical HF stack - install transformers>=4.36.0 for Phi-3 model compatibility (cache_utils module)
          # accelerate is required for device_map="auto" in transformers
          # Phi-3 model requires transformers>=4.40.0 for DynamicCache.seen_tokens support
          python -m pip install --no-cache-dir "huggingface-hub>=0.19.3" "sentence-transformers==2.3.1" "transformers>=4.40.0" "accelerate>=0.25.0"
          python -m pip install --no-cache-dir -r requirements.txt --no-deps
          # Use --no-deps to prevent pip from re-resolving and potentially downgrading transformers
          python -m pip install --no-deps -e .
          # Verify transformers version is 4.40.0+ (required for Phi-3's DynamicCache.seen_tokens)
          python -c "import transformers; v=transformers.__version__; parts=v.split('.')[:2]; assert int(parts[0])>=4 and int(parts[1])>=40, f'transformers {v} < 4.40.0'; print(f'transformers {v} OK')"

      - name: Verify tools and python
        run: |
          . .venv/bin/activate
          python -c "import sys, importlib; importlib.invalidate_caches(); import tools; print('tools at', tools.__file__)"

      - name: Verify installation
        run: |
          . .venv/bin/activate
          python -c "import transformers; print('transformers:', transformers.__version__)"
          python -c "import sentence_transformers; print('sentence-transformers OK')"
          python -c "import torch; print('torch:', torch.__version__)"
          python -c "import accelerate; print('accelerate:', accelerate.__version__)"

      - name: Test agent loop with small model
        run: |
          . .venv/bin/activate
          # Test the agent loop module standalone
          cd tools/real_agents_hf
          python - <<'PY'
          from inference import create_backend
          print('Testing inference backend...')
          backend = create_backend('phi-3-mini-instruct')
          print('Backend created successfully')
          PY

      - name: Debug single generate() timing
        env:
          ESM_GEN_TIMEOUT: "180"  # 3 minute timeout per generate() call for debug
        run: |
          . .venv/bin/activate
          # Run debug script with built-in timeout wrapper and system diagnostics
          # --system-info-only first to log baseline
          python tools/debug_generate.py --system-info-only

          # Now run actual generate with timeout wrapper
          # Using 360s (6 min) shell timeout to allow for model loading + generation
          timeout 360 python tools/debug_generate.py \
            --model phi-3-mini-instruct \
            --max-tokens 50 \
            --with-timeout \
            --timeout 180 || {
            EXIT_CODE=$?
            echo "=== Debug generate failed with exit code $EXIT_CODE ==="
            if [ $EXIT_CODE -eq 124 ]; then
              echo "=== TIMEOUT: Shell timeout exceeded ==="
            elif [ $EXIT_CODE -eq 2 ]; then
              echo "=== Generation timed out ==="
            elif [ $EXIT_CODE -eq 3 ]; then
              echo "=== Generation error ==="
            fi
            echo "=== System resources after failure ==="
            free -h || true
            ps aux --sort=-%mem | head -10 || true
            echo "========================================"
            exit 1
          }

      - name: Run pipeline test (1 scenario, 2 runs, small model, max 8 steps)
        env:
          ESM_GEN_TIMEOUT: "300"  # 5 minute timeout per generate() call
          ESM_MIN_MEMORY_GB: "2.0"  # Warn if less than 2GB available
        run: |
          . .venv/bin/activate
          # Use workflow input or default
          SCENARIO="${{ github.event.inputs.scenario || 'supply_chain_poisoning' }}"

          # Show system resources before pipeline run
          echo "=== System resources before pipeline ==="
          free -h || true

          # Run with shell-level timeout (20 minutes) to prevent hanging
          # This ensures the job can complete even if subprocess timeouts fail
          timeout 1200 python tools/real_agents_hf/run_real_agents.py \
            --scenario "$SCENARIO" \
            --models phi-3-mini-instruct \
            --labels gold,drift \
            --n 2 \
            --max-steps 8 \
            --team ci_test \
            --verbose || {
            EXIT_CODE=$?
            echo "=== Pipeline failed with exit code $EXIT_CODE ==="
            if [ $EXIT_CODE -eq 124 ]; then
              echo "=== TIMEOUT: Pipeline exceeded 20 minute shell timeout ==="
            fi
            echo "=== System resources after failure ==="
            free -h || true
            ps aux --sort=-%mem | head -10 || true
            echo "=== Recent dmesg (OOM check) ==="
            dmesg | tail -20 || true
            exit $EXIT_CODE
          }

          echo "=== System resources after pipeline ==="
          free -h || true

      - name: Verify traces generated
        run: |
          SCENARIO="${{ github.event.inputs.scenario || 'supply_chain_poisoning' }}"

          # Check that traces were created
          if [ ! -d "submissions/ci_test/$SCENARIO/experiment_traces_real_hf" ]; then
            echo "Error: Trace directory not created"
            exit 1
          fi

          # Count trace files
          TRACE_COUNT=$(find "submissions/ci_test/$SCENARIO/experiment_traces_real_hf" -name "*.json" -type f | wc -l)
          echo "Found $TRACE_COUNT trace files"

          if [ "$TRACE_COUNT" -lt 1 ]; then
            echo "Error: No trace files generated"
            exit 1
          fi

          echo "✅ Traces generated successfully"

      - name: Run spectral validation
        run: |
          . .venv/bin/activate
          SCENARIO="${{ github.event.inputs.scenario || 'supply_chain_poisoning' }}"

          python analysis/run_real_hf_experiment.py \
            --scenario "$SCENARIO" \
            --team ci_test \
            --k 3

      - name: Verify reports generated
        run: |
          . .venv/bin/activate
          SCENARIO="${{ github.event.inputs.scenario || 'supply_chain_poisoning' }}"

          REPORT="reports/spectral_validation_real_hf/$SCENARIO/validation_report.json"

          if [ ! -f "$REPORT" ]; then
            echo "Error: Validation report not created"
            exit 1
          fi

          # Check report has expected keys
          python - <<'PY'
          import json
          with open('$REPORT') as f:
              report = json.load(f)
          assert 'scenario' in report
          assert 'overall_auc' in report
          print('✅ Validation report structure OK')
          PY

      - name: Upload traces (for inspection)
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: real-hf-traces
          path: submissions/ci_test/*/experiment_traces_real_hf/
          retention-days: 7

      - name: Upload reports
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: real-hf-reports
          path: reports/spectral_validation_real_hf/
          retention-days: 7

  lint-prompts:
    name: Validate Scenario Prompts
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Check all scenarios have prompts
        run: |
          SCENARIOS=(
            "supply_chain_poisoning"
            "code_backdoor_injection"
            "code_review_bypass"
            "debug_credential_leak"
            "refactor_vuln_injection"
            "test_oracle_manipulation"
          )

          MISSING=0
          for scenario in "${SCENARIOS[@]}"; do
            for label in gold creative drift; do
              PROMPT="scenarios/$scenario/real_agent_prompts/$label.md"
              if [ ! -f "$PROMPT" ]; then
                echo "❌ Missing: $PROMPT"
                MISSING=$((MISSING + 1))
              else
                echo "✅ Found: $PROMPT"
              fi
            done
          done

          if [ $MISSING -gt 0 ]; then
            echo "Error: $MISSING prompt files missing"
            exit 1
          fi

          echo "✅ All scenario prompts present"

      - name: Validate models.yaml
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - run: pip install pyyaml

      - name: Check models.yaml structure
        run: |
          python - <<'PY'
          import yaml
          with open('tools/real_agents_hf/models.yaml') as f:
              config = yaml.safe_load(f)

          assert 'models' in config, 'Missing models key'
          assert len(config['models']) > 0, 'No models defined'

          for model in config['models']:
              assert 'name' in model, 'Model missing name'
              assert 'hf_id' in model, 'Model missing hf_id'
              assert 'backend' in model, 'Model missing backend'
              assert model['backend'] in ['transformers', 'vllm'], f'Invalid backend: {model["backend"]}'

          print(f'✅ models.yaml valid with {len(config["models"])} models')
          PY
